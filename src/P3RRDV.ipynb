{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 3 Recuperación ranqueada y vectorización de documentos (RRDV)\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento\n",
    "Lo primero que se llevara a cabo para poder hacer la recuperación ranqueada es la tokenizacion de los documentos y generacion del vocabulario. Ademas es importante tener en cuenta que el vocabulario debe estar ordenado, no debe contener stop-words, debe estar stemizado y normalizado\n",
    "\n",
    "* A continucion se cargan los documentos y los queries en una estructura de datos, se debe cambiar DOCUMENTS_PATH y QUERIES_PATH por la ruta donde se encuentran los documentos y los queries respectivamente\n",
    "* El archivo de salida es guardado en RRDV_RESULTS_FILE_PATH con el formato exigido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wes2015.d001.naf</td>\n",
       "      <td>William Beaumont and the Human Digestion.  Wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wes2015.d002.naf</td>\n",
       "      <td>Selma Lagerlöf and the wonderful Adventures of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wes2015.d003.naf</td>\n",
       "      <td>Ferdinand de Lesseps and the Suez Canal.  Ferd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wes2015.d004.naf</td>\n",
       "      <td>Walt Disney’s ‘Steamboat Willie’ and the Rise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wes2015.d005.naf</td>\n",
       "      <td>Eugene Wigner and the Structure of the Atomic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>wes2015.d327.naf</td>\n",
       "      <td>James Parkinson and Parkinson’s Disease.  Wood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>wes2015.d328.naf</td>\n",
       "      <td>Juan de la Cierva and the Autogiro.  Demonstra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>wes2015.d329.naf</td>\n",
       "      <td>Squire Whipple – The Father of the Iron Bridge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>wes2015.d330.naf</td>\n",
       "      <td>William Playfair and the Beginnings of Infogra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>wes2015.d331.naf</td>\n",
       "      <td>Juan Bautista de Anza and the Route to San Fra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename                                               body\n",
       "1    wes2015.d001.naf  William Beaumont and the Human Digestion.  Wil...\n",
       "2    wes2015.d002.naf  Selma Lagerlöf and the wonderful Adventures of...\n",
       "3    wes2015.d003.naf  Ferdinand de Lesseps and the Suez Canal.  Ferd...\n",
       "4    wes2015.d004.naf  Walt Disney’s ‘Steamboat Willie’ and the Rise ...\n",
       "5    wes2015.d005.naf  Eugene Wigner and the Structure of the Atomic ...\n",
       "..                ...                                                ...\n",
       "327  wes2015.d327.naf  James Parkinson and Parkinson’s Disease.  Wood...\n",
       "328  wes2015.d328.naf  Juan de la Cierva and the Autogiro.  Demonstra...\n",
       "329  wes2015.d329.naf  Squire Whipple – The Father of the Iron Bridge...\n",
       "330  wes2015.d330.naf  William Playfair and the Beginnings of Infogra...\n",
       "331  wes2015.d331.naf  Juan Bautista de Anza and the Route to San Fra...\n",
       "\n",
       "[331 rows x 2 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Rutas a definir segun la ubicacion de los archivos\n",
    "DOCUMENTS_PATH = '../data/docs-raw-texts'\n",
    "QUERIES_PATH = '../data/queries-raw-texts'\n",
    "RRDV_RESULTS_FILE_PATH = \"../data/RRDV-consultas_resultados\"\n",
    "\n",
    "def load_documents(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a Pandas DataFrame where each row represents a document in folder_path.\n",
    "    The DataFrame will have as many rows as there are documents in folder_path\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            folder_path: str\n",
    "                The path to the folder that contains the documents to load\n",
    "    \n",
    "        Returns\n",
    "        --------\n",
    "            documents: pd.DataFrame\n",
    "                Pandas DataFrame with two columns: \"filename\" and \"body\"\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    index = []\n",
    "    id = 1\n",
    "    columns = ['filename', 'body']\n",
    "    for filename in os.listdir(folder_path):\n",
    "        text = pd.read_xml(os.path.join(folder_path, filename))['raw'].tolist()[1]\n",
    "        filtered_text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "        document = [filename, filtered_text]\n",
    "        documents.append(document)\n",
    "        index.append(id)\n",
    "        id += 1\n",
    "\n",
    "    return pd.DataFrame(documents, index, columns)\n",
    "\n",
    "documents = load_documents(DOCUMENTS_PATH)\n",
    "queries = load_documents(QUERIES_PATH)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo tokenizamos el texto, para esto utilizamos el word tokenize de la libreria NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "documents['tokens'] = documents['body'].apply(word_tokenize)\n",
    "queries['tokens'] = queries['body'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos todos los signos de puntuacion, contracciones del ingles y dejamos el texto todo en minusculas (normalizar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(token_list):\n",
    "    return [token.lower() for token in token_list if (token not in string.punctuation and (len(token)>1 or token.isnumeric()))]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_punctuation(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de tokenizar, dejar todo en minusculas, quitaremos las stop words para que reduzcan el vocabulario y no afecten el resultado final. Para esto usaremos la libreria nltk y su metodo stopwords.words('english')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#TODO no se si normalizar cuente como poner todo en minusculas\n",
    "def remove_stop_words(token_list):\n",
    "    return [token for token in token_list if token not in stop_words]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_stop_words(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de eliminar las stop words se hace stemming a las palabras restantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(token_list):\n",
    "    return [stemmer.stem(token) for token in token_list]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: stemming(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto el texto de cada documento y query esta en un formato mas facil de procesar, por lo que se procede a realizar la representacion vectorial de los documentos y queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Representación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se hace la implementación para transformar el anterior dataframe en una estructura de bag of word para asi tener la frecuencia de cada palabra en cada documento y query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos el vocabulario como todos los tokens diferentes que se encuentran en los documentos y queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(documents):\n",
    "    vocabulary = set()\n",
    "    for tokens in documents['tokens']:\n",
    "        vocabulary.update(tokens)\n",
    "    return vocabulary\n",
    "vocabulary = extract_vocabulary(documents)\n",
    "sorted_vocabulary = sorted(vocabulary)\n",
    "bag_of_words_index = {word: index for index, word in enumerate(sorted_vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego creamos la función que nos permite pasar del dataframe de documentos a un bag of words donde cada fila es un documento y cada columna es una palabra del vocabulario, y el valor de cada celda es la frecuencia de esa palabra en ese documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def create_bow(documents: pd.DataFrame) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates the inverted index for a document set.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        documents: pd.DataFrame\n",
    "            A Pandas DataFrame that represents the document set. The\n",
    "            DataFrame should have the following columns: \"filename\", \"body\".\n",
    "            DataFrame's index should correspond to the document id\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        bow: numpy matrix\n",
    "            A numpy matrix that represents the tf-idf matrix, each column represents a document vector.\n",
    "            Each row represents a term in the vocabulary.\n",
    "            Each value represents the frequency of the term in the document.\n",
    "    \"\"\"\n",
    "    bow = [[0 for i in range(len(documents)+1)] for j in range(len(vocabulary))]\n",
    "    bow = np.array(bow)\n",
    "    print(bow)\n",
    "    for id, document in documents.iterrows():\n",
    "        for token in document['tokens']:\n",
    "            bow[bag_of_words_index[token]][id] += 1\n",
    "    \n",
    "    return bow\n",
    "\n",
    "bow = create_bow(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguente paso es crear la matriz tf-idf, que contendrá los valores td-idf para cada token-documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf_matrix(bow: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates the tf-idf matrix\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        bow: numpy matrix\n",
    "            A numpy matrix that contains the bag of word for each document.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tf_idf_matrix: numpy matrix\n",
    "            A numpy matrix that represents the tf-idf matrix, each column represents a document vector.\n",
    "            Each row represents a term in the vocabulary.\n",
    "            Each value represents the tf-idf value of the term in the document.\n",
    "    \"\"\"\n",
    "    tf_idf_matrix = np.zeros((len(vocabulary),len(documents)+1))\n",
    "    documental_frecuency = np.count_nonzero(bow, axis=1)\n",
    "    total_documents = float(len(bow[0])-1)\n",
    "    for i in range(len(bow)):\n",
    "        df = float(documental_frecuency[i])\n",
    "        for j in range(len(bow[i])):\n",
    "            tf = float(bow[i][j])\n",
    "            tf_idf_matrix[i][j] = math.log10(1+tf) * math.log10(total_documents/df)\n",
    "    return tf_idf_matrix\n",
    "\n",
    "tf_idf_matrix = create_tf_idf_matrix(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presenta la implementación del modelo de recuperación ranqueada con el método de similitud coseno. Para esto se crea una función que recibe como parámetros 2 vectores y calcula la similitud coseno entre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(vector1: np.array,vector2:np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine distance between two vectors\n",
    "    Params \n",
    "    ------\n",
    "        vector1: numpy array\n",
    "            A numpy array that represents a vector\n",
    "        vector2: numpy array\n",
    "            A numpy array that represents a vector\n",
    "    Returns\n",
    "    -------\n",
    "        cosine_distance: float  \n",
    "            A float that represents the cosine distance between vector1 and vector2\n",
    "    \"\"\"\n",
    "    if np.count_nonzero(vector1) == 0 or np.count_nonzero(vector2) == 0:\n",
    "        return 0\n",
    "    cos_distance = np.dot(vector1, vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
    "    return cos_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente creamos una función que convierte un query en un vector tf-idf con la misma estructura que los documentos, esto para poder ser operado por la similitud coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_tf_idf(query:list,documents_tf_idf:np.array)->np.array:\n",
    "    \"\"\"\n",
    "    Returns the tf-idf vector for the query\n",
    "    Params\n",
    "    ------\n",
    "        query: string\n",
    "            An array of tokens that represents the query\n",
    "        documents_tf_idf: numpy matrix\n",
    "            A numpy matrix that represents the tf-idf matrix, each column represents a document vector.\n",
    "            Each row represents a term in the vocabulary.\n",
    "            Each value represents the tf-idf value of the term in the document.\n",
    "    Returns\n",
    "    -------\n",
    "        query_tf_idf: numpy array\n",
    "            A numpy array that represents the tf-idf vector for the query\n",
    "    \"\"\"\n",
    "    query_tf_idf = np.zeros(len(documents_tf_idf))\n",
    "    documental_frecuency = np.count_nonzero(documents_tf_idf, axis=1)\n",
    "    total_documents = float(len(documents_tf_idf[0])-1)\n",
    "    for token in query:\n",
    "        if token in bag_of_words_index:\n",
    "            query_tf_idf[bag_of_words_index[token]] += 1\n",
    "    for i in range(len(query_tf_idf)):\n",
    "        value = math.log10(1+query_tf_idf[i]) * math.log10(total_documents/documental_frecuency[i])\n",
    "        query_tf_idf[i] = value\n",
    "\n",
    "    return query_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presenta el algoritmo de rrdv, que consiste en calcular la similitud coseno entre el query y cada uno de los documentos, y luego ordenarlos de mayor a menor según la similitud coseno. Los documentos con similitud coseno igual a 0 no se tienen en cuenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrdv(query:list,documents_tf_idf:np.array) -> list:\n",
    "    \"\"\"\n",
    "    Returns the ordered list of documents according to cosine distance between the query and the documents\n",
    "    Params\n",
    "    ------\n",
    "        query: string\n",
    "            An array of tokens that represents the query\n",
    "        documents_tf_idf: numpy matrix\n",
    "            A numpy matrix that represents the tf-idf matrix, each column represents a document vector.\n",
    "            Each row represents a term in the vocabulary.\n",
    "            Each value represents the tf-idf value of the term in the document.\n",
    "    Returns\n",
    "    -------\n",
    "        documents_cosine_distance: list of dictionaries that contain the document id and the cosine distance\n",
    "    \"\"\"\n",
    "    query_tf_idf = query_to_tf_idf(query,documents_tf_idf)\n",
    "    documents_cosine_distance = []\n",
    "    for i in range(1,len(documents_tf_idf[0])):\n",
    "        document_tf_idf = documents_tf_idf[:,i]\n",
    "        cos_distance = cosine_distance(query_tf_idf,document_tf_idf)\n",
    "        if cos_distance > 0:\n",
    "            documents_cosine_distance.append({'id':i,'cosine_distance':cos_distance})\n",
    "    documents_cosine_distance.sort(key=lambda x: x['cosine_distance'],reverse=True)\n",
    "    return documents_cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluación\n",
    "A continuación son procesados todos los queries por el algoritmo y su resultado es guardado en un archivo de texto con el formato exigido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear output file contents\n",
    "open(RRDV_RESULTS_FILE_PATH, \"w\").close()\n",
    "\n",
    "# Loop through queries\n",
    "for i, query in queries.iterrows():\n",
    "    # Open output file\n",
    "    file = open(RRDV_RESULTS_FILE_PATH, \"a\")\n",
    "\n",
    "    query_str = query['filename'].replace('.naf', '').replace('wes2015.', '')\n",
    "    file.write(query_str + \" \")\n",
    "\n",
    "    res = rrdv(query['tokens'], tf_idf_matrix)\n",
    "\n",
    "    # Write output file\n",
    "    counter=0\n",
    "    for doc in res:\n",
    "        docId = doc['id']\n",
    "        cosine = doc['cosine_distance']\n",
    "        if docId < 10:\n",
    "            doc_str = \"d00\" + str(docId)\n",
    "        elif docId < 100:\n",
    "            doc_str = \"d0\" + str(docId)\n",
    "        else:\n",
    "            doc_str = \"d\" + str(docId)\n",
    "        file.write(f'{doc_str}:{cosine}')\n",
    "        if counter < len(res) - 1:\n",
    "            file.write(\",\")\n",
    "        counter+=1\n",
    "    if query_str != \"q46\":\n",
    "        file.write(\"\\n\")\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
