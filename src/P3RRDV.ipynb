{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 3 Recuperación ranqueada y vectorización de documentos (RRDV)\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento\n",
    "Lo primero que se llevara a cabo para poder hacer la busqueda binaria a travez de indice invertido es la tokenizacion de los documentos y generacion del vocabulario. Ademas es importante tener en cuenta que el vocabulario debe estar ordenado, no debe contener stop-words, debe estar stemizado y normalizado\n",
    "\n",
    "* A continucion se cargan los documentos y los queries en una estructura de datos, se debe cambiar document_path y query_path por la ruta donde se encuentran los documentos y los queries respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wes2015.d001.naf</td>\n",
       "      <td>William Beaumont and the Human Digestion.  Wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wes2015.d002.naf</td>\n",
       "      <td>Selma Lagerlöf and the wonderful Adventures of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wes2015.d003.naf</td>\n",
       "      <td>Ferdinand de Lesseps and the Suez Canal.  Ferd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wes2015.d004.naf</td>\n",
       "      <td>Walt Disney’s ‘Steamboat Willie’ and the Rise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wes2015.d005.naf</td>\n",
       "      <td>Eugene Wigner and the Structure of the Atomic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>wes2015.d327.naf</td>\n",
       "      <td>James Parkinson and Parkinson’s Disease.  Wood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>wes2015.d328.naf</td>\n",
       "      <td>Juan de la Cierva and the Autogiro.  Demonstra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>wes2015.d329.naf</td>\n",
       "      <td>Squire Whipple – The Father of the Iron Bridge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>wes2015.d330.naf</td>\n",
       "      <td>William Playfair and the Beginnings of Infogra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>wes2015.d331.naf</td>\n",
       "      <td>Juan Bautista de Anza and the Route to San Fra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename                                               body\n",
       "1    wes2015.d001.naf  William Beaumont and the Human Digestion.  Wil...\n",
       "2    wes2015.d002.naf  Selma Lagerlöf and the wonderful Adventures of...\n",
       "3    wes2015.d003.naf  Ferdinand de Lesseps and the Suez Canal.  Ferd...\n",
       "4    wes2015.d004.naf  Walt Disney’s ‘Steamboat Willie’ and the Rise ...\n",
       "5    wes2015.d005.naf  Eugene Wigner and the Structure of the Atomic ...\n",
       "..                ...                                                ...\n",
       "327  wes2015.d327.naf  James Parkinson and Parkinson’s Disease.  Wood...\n",
       "328  wes2015.d328.naf  Juan de la Cierva and the Autogiro.  Demonstra...\n",
       "329  wes2015.d329.naf  Squire Whipple – The Father of the Iron Bridge...\n",
       "330  wes2015.d330.naf  William Playfair and the Beginnings of Infogra...\n",
       "331  wes2015.d331.naf  Juan Bautista de Anza and the Route to San Fra...\n",
       "\n",
       "[331 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Rutas a definir segun la ubicacion de los archivos\n",
    "DOCUMENTS_PATH = '../data/docs-raw-texts'\n",
    "QUERIES_PATH = '../data/queries-raw-texts'\n",
    "QUERIES_RESULTS_FILE_PATH = \"../data/BSII-AND-queries_results.tsv\"\n",
    "\n",
    "def load_documents(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a Pandas DataFrame where each row represents a document in folder_path.\n",
    "    The DataFrame will have as many rows as there are documents in folder_path\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            folder_path: str\n",
    "                The path to the folder that contains the documents to load\n",
    "    \n",
    "        Returns\n",
    "        --------\n",
    "            documents: pd.DataFrame\n",
    "                Pandas DataFrame with two columns: \"filename\" and \"body\"\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    index = []\n",
    "    id = 1\n",
    "    columns = ['filename', 'body']\n",
    "    for filename in os.listdir(folder_path):\n",
    "        text = pd.read_xml(os.path.join(folder_path, filename))['raw'].tolist()[1]\n",
    "        filtered_text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "        document = [filename, filtered_text]\n",
    "        documents.append(document)\n",
    "        index.append(id)\n",
    "        id += 1\n",
    "\n",
    "    return pd.DataFrame(documents, index, columns)\n",
    "\n",
    "documents = load_documents(DOCUMENTS_PATH)\n",
    "queries = load_documents(QUERIES_PATH)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo tokenizamos el texto, para esto utilizamos el word tokenize de la libreria NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "documents['tokens'] = documents['body'].apply(word_tokenize)\n",
    "queries['tokens'] = queries['body'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos todos los signos de puntuacion, contracciones del ingles y dejamos el texto todo en minusculas (normalizar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(token_list):\n",
    "    return [token.lower() for token in token_list if (token not in string.punctuation and (len(token)>1 or token.isnumeric()))]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_punctuation(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de tokenizar, dejar todo en minusculas, quitaremos las stop words para que reduzcan el vocabulario y no afecten el resultado final. Para esto usaremos la libreria nltk y su metodo stopwords.words('english')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#TODO no se si normalizar cuente como poner todo en minusculas\n",
    "def remove_stop_words(token_list):\n",
    "    return [token for token in token_list if token not in stop_words]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_stop_words(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de eliminar las stop words se hace stemming a las palabras restantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(token_list):\n",
    "    return [stemmer.stem(token) for token in token_list]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: stemming(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto el texto de cada documento y query esta en un formato mas facil de procesar, por lo que se procede a realizar la representacion vectorial de los documentos y queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Representación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se hace la implementación para transformar el anterior dataframe en una estructura de bag of word para asi tener la frecuencia de cada palabra en cada documento y query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos el vocabulario como todos los tokens diferentes que se encuentran en los documentos y queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(documents):\n",
    "    vocabulary = set()\n",
    "    for tokens in documents['tokens']:\n",
    "        vocabulary.update(tokens)\n",
    "    return vocabulary\n",
    "vocabulary = extract_vocabulary(documents)\n",
    "sorted_vocabulary = sorted(vocabulary)\n",
    "bag_of_words_index = {word: index for index, word in enumerate(sorted_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def create_bow(documents: pd.DataFrame) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates the inverted index for a document set.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        documents: pd.DataFrame\n",
    "            A Pandas DataFrame that represents the document set. The\n",
    "            DataFrame should have the following columns: \"filename\", \"body\".\n",
    "            DataFrame's index should correspond to the document id\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        bow: numpy matrix\n",
    "            A numpy matrix that represents the tf-idf matrix, each column represents a document vector.\n",
    "            Each row represents a term in the vocabulary.\n",
    "            Each value represents the frequency of the term in the document.\n",
    "    \"\"\"\n",
    "    bow = [[0 for i in range(len(documents)+1)] for j in range(len(vocabulary))]\n",
    "    bow = np.array(bow)\n",
    "    print(bow)\n",
    "    for id, document in documents.iterrows():\n",
    "        for token in document['tokens']:\n",
    "            bow[bag_of_words_index[token]][id] += 1\n",
    "    \n",
    "    return bow\n",
    "\n",
    "bow = create_bow(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf_matrix(bow: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates the tf-idf matrix\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        bow: numpy matrix\n",
    "            A numpy matrix that contains the bag of word for each document.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tf_idf_matrix: numpy matrix\n",
    "            A numpy matrix that represents the tf-idf matrix, each column represents a document vector.\n",
    "            Each row represents a term in the vocabulary.\n",
    "            Each value represents the tf-idf value of the term in the document.\n",
    "    \"\"\"\n",
    "    tf_idf_matrix = bow.copy()\n",
    "    documental_frecuency = np.count_nonzero(bow, axis=1)\n",
    "    total_documents = float(len(bow[0])-1)\n",
    "    for i in range(len(bow)):\n",
    "        df = float(documental_frecuency[i])\n",
    "        for j in range(len(bow[i])):\n",
    "            tf = float(bow[i][j])\n",
    "            tf_idf_matrix[i][j] = math.log10(1+tf) * math.log10(total_documents/df)\n",
    "    return tf_idf_matrix\n",
    "\n",
    "tf_idf_matrix = create_tf_idf_matrix(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(vector1: np.array,vector2:np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine distance between two vectors\n",
    "    Params \n",
    "    ------\n",
    "        vector1: numpy array\n",
    "            A numpy array that represents a vector\n",
    "        vector2: numpy array\n",
    "            A numpy array that represents a vector\n",
    "    Returns\n",
    "    -------\n",
    "        cosine_distance: float  \n",
    "            A float that represents the cosine distance between vector1 and vector2\n",
    "    \"\"\"\n",
    "    return np.dot(vector1, vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inverted_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m     file\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39mq\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[39m# Perform AND query with all the terms in the query\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m res \u001b[39m=\u001b[39m and_search(query[\u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m], inverted_index)\n\u001b[0;32m    100\u001b[0m \u001b[39m# Write output file\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m docId \u001b[39min\u001b[39;00m res:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inverted_index' is not defined"
     ]
    }
   ],
   "source": [
    "def and_intersect(postings1: list, postings2: list) -> list:\n",
    "    \"\"\"\n",
    "    Returns the intersection of two postings lists\n",
    "    \"\"\"\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    intersection = []\n",
    "\n",
    "    # Merge algorith taken from the book\n",
    "    while(i < len(postings1) and j < len(postings2)):\n",
    "        docId1 = postings1[i]\n",
    "        docId2 = postings2[j]\n",
    "        if docId1 == docId2:\n",
    "            intersection.append(docId1)\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif docId1 < docId2:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return intersection\n",
    "\n",
    "\n",
    "def and_search(terms: list, inverted_index: dict) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list with the ids if the documents that\n",
    "    contain all of the terms in terms list.\n",
    "\n",
    "        Params\n",
    "        ------\n",
    "            terms: list[str]\n",
    "                list of terms to look for in the documents\n",
    "            \n",
    "            inverted_index: dict\n",
    "                Inverted index created from the document base\n",
    "    \"\"\"\n",
    "    term_df_list = []\n",
    "\n",
    "    for term in terms:\n",
    "        if term in inverted_index:\n",
    "            term_df_list.append({\n",
    "                \"term\": term,\n",
    "                \"df\": inverted_index[term][\"df\"]\n",
    "            })\n",
    "        else:\n",
    "            # If a term that is not in the inverted index\n",
    "            # is found. That means that there is no document\n",
    "            # in the document base that meets the query.\n",
    "            # Hence, an empty array is returned\n",
    "            return []\n",
    "        \n",
    "    # If there is only one term to match, the function\n",
    "    # returns the postings of that term\n",
    "    if len(term_df_list) == 1:\n",
    "        return inverted_index[term_df_list[0][\"term\"]][\"postings\"]\n",
    "\n",
    "    # Sort term_df_list based on df\n",
    "    term_df_list.sort(key=lambda elem: elem[\"df\"])\n",
    "\n",
    "    # Initialize intersection as the smallest postings list\n",
    "    intersection = inverted_index[term_df_list[0][\"term\"]][\"postings\"]\n",
    "\n",
    "    for i in range(1, len(term_df_list)):\n",
    "        # If there are no items in the current intersection\n",
    "        # there is no point in calculating the intersection\n",
    "        # for the rest of the postings.\n",
    "        # Hence, the function returns current (empty) intersection\n",
    "        if len(intersection) == 0:\n",
    "            return intersection\n",
    "        \n",
    "        postings_i = inverted_index[term_df_list[i][\"term\"]][\"postings\"]\n",
    "\n",
    "        # calculate the intersection of current intersection with the next\n",
    "        # smallest posting list\n",
    "        intersection = and_intersect(intersection, postings_i)\n",
    "\n",
    "    return intersection\n",
    "\n",
    "# query processing\n",
    "\n",
    "# Clear output file contents\n",
    "open(QUERIES_RESULTS_FILE_PATH, \"w\").close()\n",
    "\n",
    "# Loop through queries\n",
    "for i, query in queries.iterrows():\n",
    "    # Open output file\n",
    "    file = open(QUERIES_RESULTS_FILE_PATH, \"a\")\n",
    "\n",
    "    if(i < 10):\n",
    "        file.write(\"q0\" + str(i) + \" \")\n",
    "    else:\n",
    "        file.write(\"q\" + str(i) + \" \")\n",
    "\n",
    "    # Perform AND query with all the terms in the query\n",
    "    res = and_search(query[\"tokens\"], inverted_index)\n",
    "\n",
    "    # Write output file\n",
    "    for docId in res:\n",
    "        if docId < 10:\n",
    "            doc_str = \"d00\" + str(docId)\n",
    "        elif docId < 100:\n",
    "            doc_str = \"d0\" + str(docId)\n",
    "        else:\n",
    "            doc_str = \"d\" + str(docId)\n",
    "        file.write(doc_str)\n",
    "        if docId != res[len(res) - 1]:\n",
    "            file.write(\",\")\n",
    "    \n",
    "    file.write(\"\\n\")\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
