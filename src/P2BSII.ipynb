{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 2 Búsqueda binaria usando índice invertido (BSII)\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento\n",
    "Lo primero que se llevara a cabo para poder hacer la busqueda binaria a travez de indice invertido es la tokenizacion de los documentos y generacion del vocabulario. Ademas es importante tener en cuenta que el vocabulario debe estar ordenado, no debe contener stop-words, debe estar stemizado y normalizado\n",
    "\n",
    "* A continucion se cargan los documentos y los queries en una estructura de datos, se debe cambiar document_path y query_path por la ruta donde se encuentran los documentos y los queries respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta que se debe cambiar segun la ubicacion del archivo, al descomprimir el zip no deberia afectar\n",
    "\n",
    "documents_path = '../data/docs-raw-texts'\n",
    "queries_path = '../data/queries-raw-texts'\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    document = [[0,'','']]\n",
    "    id = 1\n",
    "    for filename in os.listdir(folder_path):\n",
    "        text = pd.read_xml(os.path.join(folder_path, filename))['raw'].tolist()[1]\n",
    "        filtered_text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "        row = [id, filename, filtered_text]\n",
    "        document.append(row)\n",
    "        id += 1\n",
    "    return document\n",
    "\n",
    "documents = load_documents(documents_path)\n",
    "queries = load_documents(queries_path)\n",
    "\n",
    "documents = pd.DataFrame(documents)\n",
    "queries = pd.DataFrame(queries)\n",
    "\n",
    "documents = documents.rename(columns = {0:'id',1:'filename',2:'text'})\n",
    "queries = queries.rename(columns = {0:'id',1:'filename',2:'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo tokenizamos el texto, para esto utilizamos el word tokenize de la libreria NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "documents['tokens'] = documents['text'].apply(word_tokenize)\n",
    "queries['tokens'] = queries['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos todos los signos de puntuacion, contracciones del ingles y dejamos el texto todo en minusculas (normalizar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id          filename                                               text  \\\n",
      "0      0                                                                        \n",
      "1      1  wes2015.d001.naf  William Beaumont and the Human Digestion.  Wil...   \n",
      "2      2  wes2015.d002.naf  Selma Lagerlöf and the wonderful Adventures of...   \n",
      "3      3  wes2015.d003.naf  Ferdinand de Lesseps and the Suez Canal.  Ferd...   \n",
      "4      4  wes2015.d004.naf  Walt Disney’s ‘Steamboat Willie’ and the Rise ...   \n",
      "..   ...               ...                                                ...   \n",
      "327  327  wes2015.d327.naf  James Parkinson and Parkinson’s Disease.  Wood...   \n",
      "328  328  wes2015.d328.naf  Juan de la Cierva and the Autogiro.  Demonstra...   \n",
      "329  329  wes2015.d329.naf  Squire Whipple – The Father of the Iron Bridge...   \n",
      "330  330  wes2015.d330.naf  William Playfair and the Beginnings of Infogra...   \n",
      "331  331  wes2015.d331.naf  Juan Bautista de Anza and the Route to San Fra...   \n",
      "\n",
      "                                                tokens  \n",
      "0                                                   []  \n",
      "1    [william, beaumont, and, the, human, digestion...  \n",
      "2    [selma, lagerlöf, and, the, wonderful, adventu...  \n",
      "3    [ferdinand, de, lesseps, and, the, suez, canal...  \n",
      "4    [walt, disney, steamboat, willie, and, the, ri...  \n",
      "..                                                 ...  \n",
      "327  [james, parkinson, and, parkinson, disease, wo...  \n",
      "328  [juan, de, la, cierva, and, the, autogiro, dem...  \n",
      "329  [squire, whipple, the, father, of, the, iron, ...  \n",
      "330  [william, playfair, and, the, beginnings, of, ...  \n",
      "331  [juan, bautista, de, anza, and, the, route, to...  \n",
      "\n",
      "[332 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(token_list):\n",
    "    return [token.lower() for token in token_list if (token not in string.punctuation and (len(token)>1 or token.isnumeric()))]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_punctuation(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de tokenizar, dejar todo en minusculas, quitaremos las stop words para que reduzcan el vocabulario y no afecten el resultado final. Para esto usaremos la libreria nltk y su metodo stopwords.words('english')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#TODO no se si normalizar cuente como poner todo en minusculas\n",
    "def remove_stop_words(token_list):\n",
    "    return [token for token in token_list if token not in stop_words]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_stop_words(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de eliminar las stop words se hace stemming a las palabras restantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(token_list):\n",
    "    return [stemmer.stem(token) for token in token_list]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: stemming(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto el texto de cada documento y query esta en un formato mas facil de procesar, por lo que se procede a realizar la representacion vectorial de los documentos y queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Representación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se hace la implementación para transformar el anterior dataframe en una estructura de indice inertido para así poder realizar busquedas binarias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14682\n"
     ]
    }
   ],
   "source": [
    "inverted_index = {}\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    document = documents.iloc[i]\n",
    "    for token in document['tokens']:\n",
    "        if token not in inverted_index:\n",
    "            inverted_index[token] = {\"df\": 0, \"postings\": []}\n",
    "        if i not in inverted_index[token]:\n",
    "            inverted_index[token][\"df\"] += 1\n",
    "            inverted_index[token][\"postings\"].append(i)\n",
    "inverted_index\n",
    "print(len(inverted_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se pudo observar en el anterior código, se crea un diccionario que almacenara el índice invertido haciendo un recorrido por cada uno de los documentos y sus tokens. Agregando así todos los tokens del vocabulario y añadiendo a cada token el listado de documentos que contienen ese token. El vocabulario final cuenta con 14682 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m             relevant_documents \u001b[39m=\u001b[39m merge(relevant_documents,inverted_index[token][\u001b[39m'\u001b[39m\u001b[39mpostings\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m relevant_documents\n\u001b[1;32m---> 27\u001b[0m \u001b[39mprint\u001b[39m(and_search(queries\u001b[39m.\u001b[39;49miloc[\u001b[39m2\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m'\u001b[39;49m],inverted_index))\n",
      "Cell \u001b[1;32mIn[12], line 24\u001b[0m, in \u001b[0;36mand_search\u001b[1;34m(tokens, inverted_index)\u001b[0m\n\u001b[0;32m     22\u001b[0m         relevant_documents \u001b[39m=\u001b[39m inverted_index[token][\u001b[39m'\u001b[39m\u001b[39mpostings\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m         relevant_documents \u001b[39m=\u001b[39m merge(relevant_documents,inverted_index[token][\u001b[39m'\u001b[39m\u001b[39mpostings\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m relevant_documents\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merge' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO funcion para and y algoritmo de merge\n",
    "\n",
    "def organize_tokens(tokens,inverted_index):\n",
    "    organized_tokens = []\n",
    "    dfs = []\n",
    "    for token in tokens:\n",
    "        dfs.append(inverted_index[token]['df'])     \n",
    "    dfs.sort()\n",
    "    for df in dfs:\n",
    "        for token in tokens:\n",
    "            if inverted_index[token]['df'] == df:\n",
    "                organized_tokens.append(token) \n",
    "                tokens.remove(token)\n",
    "    return organized_tokens   \n",
    "\n",
    "def and_search(tokens,inverted_index):\n",
    "    organized_tokens = organize_tokens(tokens,inverted_index)\n",
    "    relevant_documents = []\n",
    "    token_document_index = [0 for i in range(len(organized_tokens))]\n",
    "    while token_document_index[0] < len(inverted_index[organized_tokens[0]]['postings']):\n",
    "        document = inverted_index[organized_tokens[0]]['postings'][token_document_index[0]]\n",
    "        document_is_relevant = True\n",
    "        token_index = 1\n",
    "        while document_is_relevant:\n",
    "            document_index=0\n",
    "            while token_document_index[document_index] < len(inverted_index[organized_tokens[document_index]]['postings']) and inverted_index[organized_tokens[document_index]]['postings'][token_document_index[document_index]] < document:\n",
    "            if inverted_index[organized_tokens[token_index]]['postings'][array_index[token_index]] == document:\n",
    "                token_index += 1\n",
    "                if token_index == len(organized_tokens):\n",
    "                    relevant_documents.append(document)\n",
    "                    document_is_relevant = False\n",
    "\n",
    "        token_document_index[0] += 1\n",
    "        \n",
    "\n",
    "print(and_search(queries.iloc[2]['tokens'],inverted_index))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
